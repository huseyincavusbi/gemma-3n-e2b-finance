{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "y3zJW6IlZhL9",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install latest transformers for Gemma 3N\n",
        "!pip install --no-deps --upgrade transformers # Only for Gemma 3N\n",
        "!pip install --no-deps --upgrade timm # Only for Gemma 3N"
      ],
      "metadata": {
        "trusted": true,
        "id": "CuhY6axH6sTc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load the Model\n",
        "max_seq_length = 2048\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    dtype = None, # Will default to torch.bfloat16 if available\n",
        ")\n",
        "\n",
        "# 2. Configure LoRA Adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank of the adapters. A common choice.\n",
        "    lora_alpha = 16, # A scaling factor for the adapters.\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 42,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        ")\n",
        "\n",
        "print(\"Unsloth model configured for 4-bit LoRA fine-tuning!\")\n"
      ],
      "metadata": {
        "id": "FrYdrNcHaLAb",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Datasets and Merge them\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "def load_and_merge_finance_datasets():\n",
        "    print(\"Loading gbharti/wealth-alpaca_lora dataset...\")\n",
        "    wealth_ds = load_dataset(\"gbharti/wealth-alpaca_lora\", split=\"train\")\n",
        "\n",
        "    print(\"Loading Josephgflowers/Finance-Instruct-500k dataset...\")\n",
        "    finance_ds = load_dataset(\"Josephgflowers/Finance-Instruct-500k\", split=\"train\")\n",
        "\n",
        "    def preprocess_wealth_alpaca(example):\n",
        "        if example.get('input'):\n",
        "            example['instruction'] = f\"{example['instruction']}\\n{example['input']}\"\n",
        "        return {\"instruction\": example[\"instruction\"], \"output\": example[\"output\"]}\n",
        "\n",
        "    def preprocess_finance_instruct(example):\n",
        "        # The output should come from the 'assistant' column in the dataset\n",
        "        return {\"instruction\": example[\"user\"], \"output\": example[\"assistant\"]}\n",
        "\n",
        "    wealth_ds = wealth_ds.map(preprocess_wealth_alpaca, remove_columns=wealth_ds.column_names)\n",
        "    finance_ds = finance_ds.map(preprocess_finance_instruct, remove_columns=finance_ds.column_names)\n",
        "\n",
        "    print(\"Merging the datasets...\")\n",
        "    merged_dataset = concatenate_datasets([wealth_ds, finance_ds])\n",
        "    return merged_dataset\n",
        "\n",
        "merged_dataset = load_and_merge_finance_datasets()\n",
        "print(\"Merged the datasets!\")"
      ],
      "metadata": {
        "id": "nZF2BS9LIq4J",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data for Gemma 3 format\n",
        "\n",
        "# We create a new column 'text' that contains the formatted prompt.\n",
        "# SFTTrainer will then use this column for training.\n",
        "def formatting_prompts_func(example):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
        "    ]\n",
        "    # The tokenizer formats the messages into the required ChatML string.\n",
        "    # We don't tokenize here, just create the formatted text string.\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "    return { \"text\": text }\n",
        "\n",
        "dataset = merged_dataset.map(formatting_prompts_func)\n",
        "\n",
        "print(\"\\n--- Formatted Dataset Example ---\")\n",
        "print(dataset[0][\"text\"])"
      ],
      "metadata": {
        "id": "HFlZlRSofHSJ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LoRA and Start Training\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4, # Effective batch size = 2 * 4 = 8\n",
        "    warmup_steps = 10,\n",
        "    max_steps = 1000, # For demo\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),\n",
        "    bf16 = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps = 1,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 42,\n",
        "    output_dir = \"outputs\",\n",
        "    report_to = \"none\",\n",
        ")\n",
        "\n",
        "# --- Initialize Trainer ---\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\", # Point trainer to our formatted 'text' column\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = training_args,\n",
        ")"
      ],
      "metadata": {
        "id": "vAeqKPaB3N5L",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start Fine-tuning ---\n",
        "print(\"Starting the fine-tuning process...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"Fine-tuning complete!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "r4mYyHi46sTd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Inference and Saving the Model\n",
        "\n",
        "print(\"\\n--- Running Inference ---\")\n",
        "from transformers import pipeline\n",
        "\n",
        "# Use Unsloth's fast inference pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Create a test prompt\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What are the main risks associated with investing in emerging markets?\"},\n",
        "]\n",
        "\n",
        "# Get the response\n",
        "outputs = pipe(messages, max_new_tokens=256, do_sample=True, temperature=0.7, top_p=0.95)\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "yBT7xhprF1ZE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to Hugging Face Hub\n",
        "notebook_login()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Z2EhVdrGF1ZE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Adapters and Push to Hugging Face Hub\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Save the fine-tuned LoRA adapters\n",
        "print(\"\\n--- Saving LoRA Adapters ---\")\n",
        "model.save_pretrained(\"gemma-3n-e2b-finance-loraa\")\n",
        "tokenizer.save_pretrained(\"gemma-3n-e2b-finance-lora\")\n",
        "print(\"Model adapters saved to 'gemma-3n-e2b-finance-lora'\")\n",
        "\n",
        "# Push the model adapters and tokenizer to the Hub\n",
        "repo_name = \"huseyincavus/gemma-3n-e2b-finance-lora\"\n",
        "\n",
        "print(f\"\\n--- Pushing LoRA Adapters to Hugging Face Hub ({repo_name}) ---\")\n",
        "model.push_to_hub(repo_name, token = True)\n",
        "tokenizer.push_to_hub(repo_name, token = True)\n",
        "print(\"Model adapters and tokenizer pushed to Hugging Face Hub!\")\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "qoTuxoAbF1ZE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "AFTER RESTART"
      ],
      "metadata": {
        "id": "FjRBElZhF1ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge base model + adapters and push to Hub\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Define the base model and LoRA adapter\n",
        "base_model_id = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n",
        "lora_adapter_id = \"huseyincavus/gemma-3n-e2b-finance-lora\"\n",
        "merged_model_id = \"huseyincavus/gemma-3n-e2b-finance-merged\"\n",
        "\n",
        "# Load the base model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load the LoRA adapter and merge it with the base model\n",
        "model = PeftModel.from_pretrained(model, lora_adapter_id)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Push the merged model and tokenizer to the Hugging Face Hub\n",
        "model.push_to_hub(merged_model_id)\n",
        "tokenizer.push_to_hub(merged_model_id)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-01T09:54:40.932797Z",
          "iopub.execute_input": "2025-08-01T09:54:40.933089Z",
          "iopub.status.idle": "2025-08-01T09:57:40.311120Z",
          "shell.execute_reply.started": "2025-08-01T09:54:40.933064Z",
          "shell.execute_reply": "2025-08-01T09:57:40.310348Z"
        },
        "id": "dOipj4kWF1ZF"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}